---
layout: post
title:  "Transformer explanations: a collection"
date:   2023-10-09 07:58:38 +0200
comments: true
---

Transformer is a powerful architecture that can be difficult to understand. There are many great explanations on the web, each approaching the subject in a different way. Here I link the explanations I liked, and mention who I believe the target audience is for each one.
<!--more-->

The goal is to provide a collection of links for you to choose from, but reading all of them is still helpful to engage with the concept from different perspectives & to cement your knowledge.
# [Transformers from scratch](https://peterbloem.nl/blog/transformers)

*Target audience:* people with Machine Learning background

*Comment/opinion:* An all-around outstanding explanation that includes clear code & excellent illustrations. A personal favorite.
# [The transformer … “explained”?](https://nostalgebraist.tumblr.com/post/185326092369/the-transformer-explained)

*Target audience:* people with general Computer Science background

*Comment/opinion:* Excellent overview, motivation, and intuition. No pictures. No math. Short.
# [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)

*Target audience:* mathematically-minded ML people

*Comment/opinion:* Very nice & clear formalism. No pictures.
# [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

*Target audience:* ML people who know what an embedding is

*Comment/opinion:* Great illustrations. Explanation of self-attention was not intuitively clear to me.
# [Transformer - Illustration and code](https://github.com/vinsis/math-and-ml-notes/blob/master/notebooks/Transformer%20-%20Illustration%20and%20code.ipynb)

*Target audience:* ML people who know what an embedding is & find reading code helpful

*Description by the author:* "This notebook combines the excellent [illustration](http://jalammar.github.io/illustrated-transformer/) of the [transfomer](https://arxiv.org/abs/1706.03762) by Jay Alammar and the [code annonation](http://nlp.seas.harvard.edu/2018/04/03/attention.html) by `harvardnlp` lab."

*Opinion:* Reading the code was very helpful for me. Math not rendered nicely. Should be read after [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/). 

# [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer)

*Target audience:* ML people who know what an embedding is, find reading code helpful, and are interested in full details of the original Transformer paper

*Comment/opinion:* This is a rearranged version of the paper intermingled with the code. Extensive. Math rendered nicely. Illustrations are ok.

That's it! If you have suggestions on what else to include, send me an email :)