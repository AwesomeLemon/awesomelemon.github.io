<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-25T21:57:48+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alexander Chebykin</title><subtitle></subtitle><entry><title type="html">Install nvcc without root</title><link href="http://localhost:4000/2023/08/25/nvcc-no-root.html" rel="alternate" type="text/html" title="Install nvcc without root" /><published>2023-08-25T17:58:38+02:00</published><updated>2023-08-25T17:58:38+02:00</updated><id>http://localhost:4000/2023/08/25/nvcc-no-root</id><content type="html" xml:base="http://localhost:4000/2023/08/25/nvcc-no-root.html">&lt;p&gt;I needed to install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvcc&lt;/code&gt; on our group server, where I lack root privileges. I found a nice &lt;a href=&quot;https://github.com/pyg-team/pytorch_geometric/issues/392#issuecomment-503335625&quot;&gt;guide&lt;/a&gt;, in this post I will slightly expand on it by explicitly mentioning every step I had to take. Hopefully this will make life easier for future-me and for my colleagues :)&lt;/p&gt;

&lt;p&gt;Download a &lt;a href=&quot;https://developer.nvidia.com/cuda-downloads&quot;&gt;runfile&lt;/a&gt; for your OS:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda_11.7.0_515.43.04_linux.run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On our servers there is not enough space in /tmp for the next step, so:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export TMPDIR='/export/scratch2/data/aleksand/tmp'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Install toolkit only (assumes that the drivers are already installed)&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash cuda_11.7.0_515.43.04_linux.run --silent --override --toolkit --toolkitpath=/export/scratch2/data/aleksand/cuda117
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, export three paths (you can also add them to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;)&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export LD_LIBRARY_PATH=/export/scratch2/data/aleksand/cuda117/lib64:$LD_LIBRARY_PATH
export PATH=/export/scratch2/data/aleksand/cuda117/bin:$PATH
export CPATH=/export/scratch2/data/aleksand/cuda117/include:$CPATH
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;P.S. My use case (StyleGAN 2) needed gcc (and g++) version &amp;lt; 11, they can be easily installed via conda:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install -c conda-forge gcc=10.4.0 gxx=10.4.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">I needed to install nvcc on our group server, where I lack root privileges. I found a nice guide, in this post I will slightly expand on it by explicitly mentioning every step I had to take. Hopefully this will make life easier for future-me and for my colleagues :)</summary></entry><entry><title type="html">ENCAS: Search cascades of neural networks in any model pool</title><link href="http://localhost:4000/2022/07/12/Evolutionary-neural-cascade-search-across-supernetworks.html" rel="alternate" type="text/html" title="ENCAS: Search cascades of neural networks in any model pool" /><published>2022-07-12T17:58:38+02:00</published><updated>2022-07-12T17:58:38+02:00</updated><id>http://localhost:4000/2022/07/12/Evolutionary-neural-cascade-search-across-supernetworks</id><content type="html" xml:base="http://localhost:4000/2022/07/12/Evolutionary-neural-cascade-search-across-supernetworks.html">&lt;p&gt;&lt;em&gt;Based on our paper &lt;a href=&quot;https://arxiv.org/abs/2203.04011&quot;&gt;“Evolutionary Neural Cascade Search across Supernetworks”&lt;/a&gt; (Best Paper award @ GECCO 2022)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Search for cascades of neural networks in model pools of hundreds of models, optionally using Neural Architecture Search to efficiently generate many diverse task-specific neural networks. Get dominating trade-off fronts on ImageNet and CIFAR-10/100, improved ImageNet SOTA (of publicly available models).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table of contents:&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#motivation&quot; id=&quot;markdown-toc-motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#background-ensembles--cascades&quot; id=&quot;markdown-toc-background-ensembles--cascades&quot;&gt;Background: ensembles &amp;amp; cascades&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#where-can-we-get-models-for-our-cascades&quot; id=&quot;markdown-toc-where-can-we-get-models-for-our-cascades&quot;&gt;Where can we get models for our cascades?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#background-neural-architecture-search-nas&quot; id=&quot;markdown-toc-background-neural-architecture-search-nas&quot;&gt;Background: Neural Architecture Search (NAS)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#different-supernetworks-give-different-trade-off-fronts&quot; id=&quot;markdown-toc-different-supernetworks-give-different-trade-off-fronts&quot;&gt;Different supernetworks give different trade-off fronts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#how-can-we-search-for-cascades&quot; id=&quot;markdown-toc-how-can-we-search-for-cascades&quot;&gt;How can we search for cascades?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experiment-setup&quot; id=&quot;markdown-toc-experiment-setup&quot;&gt;Experiment setup&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dominating-trade-off-front-on-imagenet&quot; id=&quot;markdown-toc-dominating-trade-off-front-on-imagenet&quot;&gt;Dominating trade-off front on ImageNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nas-discovered-networks-are-improved-by-cascading&quot; id=&quot;markdown-toc-nas-discovered-networks-are-improved-by-cascading&quot;&gt;NAS-discovered networks are improved by cascading&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#five-supernetworks-are-better-than-one&quot; id=&quot;markdown-toc-five-supernetworks-are-better-than-one&quot;&gt;Five supernetworks are better than one&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#encas-outperforms-efficient-nas-approaches&quot; id=&quot;markdown-toc-encas-outperforms-efficient-nas-approaches&quot;&gt;ENCAS outperforms efficient NAS approaches&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;We would always like for our neural networks to be as effective and efficient as possible. In practical terms, it means maximizing some measure of performance (e.g. accuracy) while minimizing some measure of used resources (e.g. FLOPs). So we find ourselves in a multi-objective setting: we want to find a trade-off front of models of varying quality and computation requirements.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/Multi-objective_trade-off_Pareto_front.drawio.png&quot; alt=&quot;mo-front&quot; style=&quot;width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background-ensembles--cascades&quot;&gt;Background: ensembles &amp;amp; cascades&lt;/h1&gt;

&lt;p&gt;One way to improve performance is by combining several models in an ensemble. Let’s say we have two models on our trade-off front:&lt;/p&gt;

&lt;!-- {:refdef: style=&quot;text-align: center;&quot;} --&gt;
&lt;p&gt;&lt;img src=&quot;/pics/encas/ens1.png&quot; alt=&quot;mo-front&quot; style=&quot;width: 60%;&quot; /&gt;
&lt;!-- {: refdef} --&gt;&lt;/p&gt;

&lt;p&gt;We can create an ensemble by passing the input to both of the models and averaging their outputs. This creates a new point on the trade-off front, with improved performance, but also increased compute requirements.&lt;/p&gt;

&lt;!-- {:refdef: style=&quot;text-align: center;&quot;} --&gt;
&lt;p&gt;&lt;img src=&quot;/pics/encas/ens2.png&quot; alt=&quot;mo-front&quot; style=&quot;width: 75%;&quot; /&gt;
&lt;!-- {: refdef} --&gt;&lt;/p&gt;

&lt;p&gt;Performance improves because, in terms of the bias-variance tradeoff, ensembling reduce variance: mistakes of different models cancel out. So ensembles &lt;a href=&quot;http://www.j-wichard.de/publications/salerno_lncs_2003.pdf&quot;&gt;benefit&lt;/a&gt; from diverse member models: for the mistakes of the ensemble members to cancel out, these mistakes have to be different!&lt;/p&gt;

&lt;!-- (An easy way to understand why different mistakes are needed for a good ensemble, is to imagine an ensemble of a model and its exact clone: since their predictions are exactly the same, averaging them doesn’t change anything, and so ensembling adds no value). --&gt;

&lt;p&gt;At the same time, efficiency suffers because we are now using two models instead of one, which obviously requires more compute.&lt;/p&gt;

&lt;p&gt;But do we actually need to use all the models on all the inputs? Not really, since there exist very simple inputs that can be classified correctly by just one model.&lt;/p&gt;

&lt;p&gt;That is the intuition behind cascading. A cascade is just an ensemble with an optional early exit.
Let’s return to our two models on the trade-off front and arrange them slightly differently:&lt;/p&gt;

&lt;!-- {:refdef: style=&quot;text-align: center;&quot;} --&gt;
&lt;p&gt;&lt;img src=&quot;/pics/encas/casc1.png&quot; alt=&quot;&quot; style=&quot;width: 85%;&quot; /&gt;
&lt;!-- {: refdef} --&gt;&lt;/p&gt;

&lt;p&gt;While in an ensemble we passed the input to both of the models, in a cascade we’ll start by passing it only to the first one.&lt;/p&gt;

&lt;p&gt;Then we can estimate the confidence of the model’s output - and if it is high enough, stop the computation and return this output as the cascade’s prediction. So if the input is easy, only the first model will be used, and compute won’t be wasted on running the second one.&lt;/p&gt;

&lt;p&gt;If, on the other hand, the input is hard, and the first model is not confident, we will pass the input to the second model, and average the outputs of the models, the same way as we did in an ensemble.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/casc1.5.png&quot; alt=&quot;&quot; style=&quot;width: 85%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And now we have our cascade, which is again a new point on the trade-off front.&lt;/p&gt;

&lt;!-- {:refdef: style=&quot;text-align: center;&quot;} --&gt;
&lt;p&gt;&lt;img src=&quot;/pics/encas/casc2.png&quot; alt=&quot;&quot; style=&quot;width: 95%;&quot; /&gt;
&lt;!-- {: refdef} --&gt;&lt;/p&gt;

&lt;p&gt;In this case not only has the performance improved (since for hard inputs a cascade is equivalent to an ensemble), but the required compute didn’t grow too much: only the first model is used for all the inputs, and the second one is used rarely, when it’s actually needed. This means that on average we save a lot of compute, and that’s great!&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;cascades&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How can we represent a cascade of two models? Well, we need to reference the two used models, and we need to define a confidence threshold for deciding whether to do an early exit:&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/casc_repr1.png&quot; alt=&quot;&quot; style=&quot;width: 30%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This trivially extends to larger cascades: for a cascade of three models we need two thresholds: the first one to decide whether to stop after the first model; and the second one to decide whether to stop after the second model.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/casc3.png&quot; alt=&quot;&quot; style=&quot;width: 95%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/casc_repr2.png&quot; alt=&quot;&quot; style=&quot;width: 55%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In general, for &lt;em&gt;&lt;strong&gt;k&lt;/strong&gt;&lt;/em&gt; models we need &lt;em&gt;&lt;strong&gt;k - 1&lt;/strong&gt;&lt;/em&gt; thresholds.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/casc_repr3.png&quot; alt=&quot;&quot; style=&quot;width: 65%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To sum up, in order to create a cascade we need to select models that work well together, and choose thresholds for deciding on an early exit.&lt;/p&gt;

&lt;h1 id=&quot;where-can-we-get-models-for-our-cascades&quot;&gt;Where can we get models for our cascades?&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Downloading pre-trained models&lt;/strong&gt; is good if they exist for the dataset you’re interested in, which is often not the case.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Training many networks manually&lt;/strong&gt; is possible but labour-intensive, and it’s usually not feasible to train a really large amount of models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Neural Architecture Search&lt;/strong&gt; allows creating hundreds of diverse task-specific models in a single run.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;background-neural-architecture-search-nas&quot;&gt;Background: Neural Architecture Search (NAS)&lt;/h1&gt;

&lt;p&gt;NAS is a paradigm of the automatic search for a task-specific neural network architecture. Architecture can be searched via many approaches, in our work we rely on Evolutionary Algorithms (EAs).&lt;/p&gt;

&lt;p&gt;An important direction of NAS research is weight sharing: for the sake of efficiency, not all possible architectures are considered but only those that are subsets of a supernetwork.&lt;/p&gt;

&lt;p&gt;In the toy supernetwork below, there are just two possible operations in each position, a subnetwork can use one of them.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/supernet1.png&quot; alt=&quot;&quot; style=&quot;width: 65%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This approach is efficient because different subnetworks that use the same operation in the same position can share the weights of this operation (and so they don’t have to be trained from scratch).&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/supernet2.png&quot; alt=&quot;&quot; style=&quot;width: 65%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the early supernetwork-based approaches (such as &lt;a href=&quot;https://arxiv.org/abs/1806.09055&quot;&gt;DARTS&lt;/a&gt;), the best subnetwork was retrained from scratch after the search. However, there now exist algorithms for training a supernetwork (e.g. &lt;a href=&quot;https://ofa.mit.edu/&quot;&gt;OFA&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2102.07954&quot;&gt;AlphaNet&lt;/a&gt;) such that the subnetworks require &lt;strong&gt;no retraining&lt;/strong&gt; - and so if you have a trained supernetwork, you can create hundreds of different models without additional costs!&lt;/p&gt;

&lt;p&gt;But the diversity of the models in a supernetwork is restricted by the design of the supernetwork, since all of the subnetworks need to be trained together.&lt;/p&gt;

&lt;p&gt;We became curious about how much the results of NAS are influenced by the manual step of choosing a supernetwork, so we investigated what happens with a prominent NAS algorithm called &lt;a href=&quot;https://arxiv.org/abs/2005.05859&quot;&gt;Neural Architecture Transfer&lt;/a&gt; (NAT). NAT is a multi-objective NAS algorithm that can adapt a pretrained supernetwork to any task with the help of an EA called &lt;a href=&quot;https://www.researchgate.net/publication/264387359_An_Evolutionary_Many-Objective_Optimization_Algorithm_Using_Reference-Point-Based_Nondominated_Sorting_Approach_Part_I_Solving_Problems_With_Box_Constraints&quot;&gt;NSGA-III&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;different-supernetworks-give-different-trade-off-fronts&quot;&gt;Different supernetworks give different trade-off fronts&lt;/h1&gt;

&lt;p&gt;We run NAT on 5 different supernetworks, and find that the resulting trade-off fronts differ a lot (here is what they look like for ImageNet):&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/diff_fronts.png&quot; alt=&quot;&quot; style=&quot;width: 35%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moreover, there is no supernetwork that covers both very small and very large models.&lt;/p&gt;

&lt;p&gt;So the manual choice of a supernetwork restricts what a NAS algorithm can find before the search even starts!&lt;/p&gt;

&lt;p&gt;This makes sense, since each supernetwork defines its own search space, with different possible operations, different numbers of layers, and so on.&lt;/p&gt;

&lt;p&gt;This also means that subnetworks coming from different supernetworks can be quite diverse. And that’s exactly what we need to create good cascades!&lt;/p&gt;

&lt;p&gt;So we can use NAT on each supernetwork separately, and then put all the found models into a single model pool. This way, we can efficiently create many task-specific models that are also diverse because they come from different supernetworks.&lt;/p&gt;

&lt;h1 id=&quot;how-can-we-search-for-cascades&quot;&gt;How can we search for cascades?&lt;/h1&gt;

&lt;p&gt;We need several ingredients to define our search procedure.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution representation&lt;/strong&gt; follows the description mentioned &lt;a href=&quot;#cascades&quot;&gt;above&lt;/a&gt;: a cascade of size &lt;strong&gt;k&lt;/strong&gt; can be represented by &lt;strong&gt;k&lt;/strong&gt; models (i.e. the indices of the models in our model pool) and &lt;strong&gt;k-1&lt;/strong&gt; confidence thresholds (we use indices of discretized values in the [0.0, 1.0] range)&lt;/p&gt;

&lt;p&gt;Our multi-objective &lt;strong&gt;fitness function&lt;/strong&gt; estimates the effectivity (e.g. accuracy) and the efficiency (e.g. FLOPs) of a cascade by evaluating it on the validation set.&lt;/p&gt;

&lt;p&gt;Any general-purpose multi-objective &lt;strong&gt;algorithm&lt;/strong&gt; can be applied to optimize the fitness function, we use &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2210650217304765&quot;&gt;MO-GOMEA&lt;/a&gt;, a well-performing EA that requires no hyperparameter tuning (and none was done in our case).&lt;/p&gt;

&lt;p&gt;Our algorithm (ENCAS) can be briefly summed up like this:&lt;/p&gt;

&lt;ol start=&quot;0&quot;&gt;
  &lt;li&gt;&lt;span style=&quot;color:#505050&quot;&gt;(optional) Create diverse models for the specific task by running NAT on many supernetworks.&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;Create a combined model pool of all the available models.&lt;/li&gt;
  &lt;li&gt;Search for cascades via MO-GOMEA.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ENCAS has several advantages:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Efficient runtime of 1 GPU-hour thanks to pre-computing the outputs of all the models (which we can do because the models are known in advance).&lt;/li&gt;
  &lt;li&gt;Usage of pre-trained weights, which are always important for good performance of neural networks.&lt;/li&gt;
  &lt;li&gt;The user only needs to set the maximum cascade size, smaller cascades can still be found. This is achieved by the inclusion of a “no operation” model that does nothing: if a cascade of e.g. size 3 includes this model, it effectively becomes a cascade of size 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;experiment-setup&quot;&gt;Experiment setup&lt;/h1&gt;

&lt;p&gt;We test our approach on CIFAR-10, CIFAR-100, and ImageNet.&lt;/p&gt;

&lt;p&gt;Note that for our ImageNet experiments we used additional data for search: we have to do it because search requires data not seen during training, but the pretrained weights that we rely on were trained with all the data. So we use ImageNetV2 for cascade evaluation during the search. Since the size of this additional data is small (it’s just 1.6% of the ImageNet), we hope that the impact of this is negligible.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:grey&quot;&gt;
(also, in our preliminary experiments we tried splitting the ImageNet validation set into two halves - one for the search, and one for reporting the results - and achieved similar results. But since in the literature the results on the whole validation set are usually reported, and we wanted our results to be easy to compare to, we decided to report the results on the whole validation set, and use ImageNetV2 for search).
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;For all the datasets, the test set was only used to report the results, it was never used to select models, and so some of our trade-off fronts are not monotonous (since the models were selected based on the validation performance, but the test performance is shown).&lt;/p&gt;

&lt;p&gt;Each experiment was run 10 times, the median run (by hypervolume) is plotted, and the area between the worst and the best trade-off fronts is shaded.&lt;/p&gt;

&lt;h1 id=&quot;dominating-trade-off-front-on-imagenet&quot;&gt;Dominating trade-off front on ImageNet&lt;/h1&gt;

&lt;p&gt;We ran ENCAS on 518 pre-trained models from &lt;a href=&quot;https://github.com/rwightman/pytorch-image-models&quot;&gt;timm&lt;/a&gt;, which is a brilliant library that includes a wide variety of models that are small (e.g. MobileNetV3@60 MFLOPS), large (e.g. BeiT@362,000 MFLOPS), and everything in between.&lt;/p&gt;

&lt;p&gt;ENCAS found a trade-off front of cascades that dominates across the board:&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/imagenet.png&quot; alt=&quot;&quot; style=&quot;width: 40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The maximum accuracy was also improved from 88.6% to 89.0%, while the amount of required compute decreased by 18%, from 362 to 296 GFLOPs.&lt;/p&gt;

&lt;p&gt;ENCAS finds hundreds of good cascades, which means you can choose one that fits your computational constraints just right.&lt;/p&gt;

&lt;p&gt;And all of this was achieved in a single run that took just one GPU-hour! 
(Well, that’s how long the search itself takes. Prior to it, all the models need to be evaluated, and their outputs stored, which takes some hours, but only needs to be done once)&lt;/p&gt;

&lt;p&gt;The ability to work in a search space of hundreds of models is important for great results. In the previous &lt;a href=&quot;https://arxiv.org/abs/2012.01988&quot;&gt;work&lt;/a&gt; that strongly inspired us, cascades of EfficientNet models were found via grid search.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/imagenet2.png&quot; alt=&quot;&quot; style=&quot;width: 40%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The resulting cascades improve the trade-off front, but it’s hard to go beyond the models available in the pool (which motivates making the pool as large as possible, which in turn prohibits the usage of inefficient approaches like grid search (to be clear, the authors of the &lt;a href=&quot;https://arxiv.org/abs/2012.01988&quot;&gt;work&lt;/a&gt; point it out themselves; the search algorithm was not the key part of that work)).&lt;/p&gt;

&lt;h1 id=&quot;nas-discovered-networks-are-improved-by-cascading&quot;&gt;NAS-discovered networks are improved by cascading&lt;/h1&gt;

&lt;p&gt;We applied our implementation of NAT to 5 supernetworks, and chose the one that produced the best trade-off front. Then we ran ENCAS on the models from this front, and achieved better trade-off fronts for all the datasets!&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/beatNat.png&quot; alt=&quot;&quot; style=&quot;width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;five-supernetworks-are-better-than-one&quot;&gt;Five supernetworks are better than one&lt;/h1&gt;

&lt;p&gt;We then applied ENCAS to the trade-off fronts of all five supernetworks instead of just the best one, and the results improved even more:&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/5beat1.png&quot; alt=&quot;&quot; style=&quot;width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This means that increasing the size &amp;amp; diversity of the model pool improves the results. Our method is able to work with models from arbitrary supernetworks, and thus make use of diverse &amp;amp; incompatible search spaces.&lt;/p&gt;

&lt;p&gt;(But I should mention that currently available supernetworks are not very diverse, and so the impact is not as big as it could be - see the paper for further discussion of this and other limitations)&lt;/p&gt;

&lt;p&gt;Still, the current good results mean that…&lt;/p&gt;

&lt;h1 id=&quot;encas-outperforms-efficient-nas-approaches&quot;&gt;ENCAS outperforms efficient NAS approaches&lt;/h1&gt;

&lt;p&gt;(in most cases)&lt;/p&gt;

&lt;p&gt;This is achieved in large part thanks to cascades and to the availability of ImageNet-pretrained supernetworks that NAT can effectively adapt to the task at hand, thus creating a good model pool for ENCAS.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/pics/encas/beatAll.png&quot; alt=&quot;&quot; style=&quot;width: 90%;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The main strength of ENCAS is its generality: any trade-off front of models can be improved by cascading - it doesn’t matter where the models came from. For example, it would be possible to outperform EfficientNet-s on CIFAR-100 by simply adding these models to the model pool of the NAT-derived models.&lt;/p&gt;

&lt;p&gt;This means that you need to simply get many models in any way you want, and then just let the search find good cascades for you, making the model on your trade-off front more effective and efficient.&lt;/p&gt;

&lt;p&gt;Our &lt;a href=&quot;https://github.com/AwesomeLemon/ENCAS&quot;&gt;code&lt;/a&gt; is publicly available, so I invite you to try it out for yourself.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/2203.04011&quot;&gt;paper&lt;/a&gt; contains more cool experiments (e.g. joint supernetwork weight adaptation and cascade architecture search) - check it out if you’re interested!&lt;/p&gt;

&lt;p&gt;To cite:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{10.1145/3512290.3528749,
	author = {Chebykin, Alexander and Alderliesten, Tanja and Bosman, Peter A. N.},
	title = {Evolutionary Neural Cascade Search across Supernetworks},
	year = {2022},
	isbn = {9781450392372},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3512290.3528749},
	doi = {10.1145/3512290.3528749},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	pages = {1038–1047},
	numpages = {10},
	series = {GECCO '22}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">Based on our paper “Evolutionary Neural Cascade Search across Supernetworks” (Best Paper award @ GECCO 2022)</summary></entry><entry><title type="html">Document recognition with Python, OpenCV and Tesseract</title><link href="http://localhost:4000/2017/01/15/Document-recognition-with-Python-OpenCV-and-Tesseract.html" rel="alternate" type="text/html" title="Document recognition with Python, OpenCV and Tesseract" /><published>2017-01-15T14:49:38+01:00</published><updated>2017-01-15T14:49:38+01:00</updated><id>http://localhost:4000/2017/01/15/Document-recognition-with-Python-OpenCV-and-Tesseract</id><content type="html" xml:base="http://localhost:4000/2017/01/15/Document-recognition-with-Python-OpenCV-and-Tesseract.html">&lt;p&gt;Recently I’ve conducted my own little experiment with the document recognition technology: I’ve successfully went from an image to the recognized editable text.
On the way I heavily relied on the two following articles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/&quot;&gt;Build a Kick-Ass Mobile Document Scanner in Just 5 Minutes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html&quot;&gt;Finding blocks of text in an image using Python, OpenCV and numpy&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However, I’ve added something myself, and that’s what I want to write about: the ways to improve upon the given articles to achieve the goal of recognizing plain text from photos at arbitrary angles and illumination.&lt;/p&gt;

&lt;h3 id=&quot;skew-removal-first-article&quot;&gt;Skew removal (first article)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;If the edges in your image aren’t recognized properly, it makes sense to move interval in Canny function to the left. For example, I’ve settled on [40, 150], instead of [75, 200] proposed in the article.&lt;/li&gt;
  &lt;li&gt;To be able to recognize documents with less straight than rectangular shapes (for example, with curved corners), you should apply blurring to the image with detected edges. I found that it improves approximation quality even in some cases of “good” document shapes, and decreases in none.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Resulting image at this point can be fed to the Tesseract engine, but to get better results out of it, we can conduct two more steps:&lt;/p&gt;

&lt;h3 id=&quot;noise-removal&quot;&gt;Noise removal&lt;/h3&gt;
&lt;p&gt;I’ve experimented with different combinations of Gaussian blur, median blur, erosion and dilation (these last two can sound scary, but they aren’t. See &lt;a href=&quot;http://docs.opencv.org/2.4/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.html&quot;&gt;Eroding and Dilating&lt;/a&gt;).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gaussian blur corrupts text more than median blur&lt;/li&gt;
  &lt;li&gt;I found that using only dilation yields better average results than any other combination of mentioned techniques.&lt;/li&gt;
  &lt;li&gt;In addition to removing noise, dilation makes text clearer (by making white spaces inside of letter such as ‘a’ or ‘e’ bigger)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cropping-second-article&quot;&gt;Cropping (second article)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;To achieve quicker processing, the image at this step can be resized not to the height of 2048, but of 600 (I wouldn’t recommend anything less than 500). I haven’t observed any negative effects after introducing this change.&lt;/li&gt;
  &lt;li&gt;Two rank filters can be replaced by a single median blur&lt;/li&gt;
  &lt;li&gt;Shape of the dilation kernel doesn’t affect the result&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now it’s time to feed the resulting image to the Tesseract. It can be done quite simply with:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;pytesseract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_to_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'scan_res.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Additionally, I want to mention the problem of measuring accuracy of recognition. Having found nothing on Google in the first five minutes of the search, I’ve coded my own metric, which turned out to be not that good. That forced me to google some more, and so I’ve found a paper that discusses and compares different metrics (on their own dataset, which is quite different from mine: Arabic handwriting isn’t exactly English words in print). It gave me the names of the metrics however, which was good enough. The paper can be found &lt;a href=&quot;http://www.ijcsi.org/papers/IJCSI-11-3-1-18-26.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ve settled on Jaro-Winkler distance, which gives sufficiently accurate results. Also it’s implemented in jellyfish package, so it’s quite easy to use.
I hope this article will prove itself useful. 
For reference, my final code can be found &lt;a href=&quot;https://github.com/AwesomeLemon/document-recognition&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="opencv" /><category term="python" /><category term="tesseract" /><category term="ocr" /><summary type="html">Recently I’ve conducted my own little experiment with the document recognition technology: I’ve successfully went from an image to the recognized editable text. On the way I heavily relied on the two following articles:</summary></entry></feed>